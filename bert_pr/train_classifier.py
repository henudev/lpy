import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.preprocessing import LabelEncoder
from datasets import Dataset
import joblib
import os

def main():
    # ------------------------
    # 1. åŠ è½½æ•°æ®
    # ------------------------
    data_path = "data_10000.csv"
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"{data_path} ä¸å­˜åœ¨ï¼Œè¯·å…ˆå‡†å¤‡æ•°æ®æ–‡ä»¶")
    data = pd.read_csv(data_path)

    # ------------------------
    # 2. æ ‡ç­¾ç¼–ç 
    # ------------------------
    label_encoder = LabelEncoder()
    data["label_id"] = label_encoder.fit_transform(data["label"])

    # ------------------------
    # 3. åˆ é™¤åŸå­—ç¬¦ä¸²åˆ—ï¼Œä¿ç•™æ•´æ•°æ ‡ç­¾
    # ------------------------
    data = data.drop(columns=["label"])
    data = data.rename(columns={"label_id": "label"})

    # ------------------------
    # 4. æ„å»º Dataset
    # ------------------------
    dataset = Dataset.from_pandas(data, preserve_index=False)

    # ------------------------
    # 5. åŠ è½½åˆ†è¯å™¨
    # ------------------------
    tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")

    # ------------------------
    # 6. æ•°æ®ç¼–ç å‡½æ•°
    # ------------------------
    def tokenize_function(examples):
        tokenized = tokenizer(
            examples["text"], padding="max_length", truncation=True, max_length=64
        )
        tokenized["labels"] = examples["label"]  # ç¡®ä¿ labels ä¸ºæ•´æ•°
        return tokenized

    tokenized_dataset = dataset.map(tokenize_function, batched=True)

    # ------------------------
    # 7. åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    # ------------------------
    tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.25)

    # ------------------------
    # 8. åŠ è½½æ¨¡å‹
    # ------------------------
    num_labels = len(label_encoder.classes_)
    model = BertForSequenceClassification.from_pretrained(
        "bert-base-chinese", num_labels=num_labels
    )

    # ------------------------
    # 9. è®­ç»ƒå‚æ•°
    # ------------------------
    training_args = TrainingArguments(
        output_dir="./results",
        evaluation_strategy="epoch",
        save_strategy="epoch",
        learning_rate=2e-5,
        per_device_train_batch_size=4,
        per_device_eval_batch_size=4,
        num_train_epochs=3,
        weight_decay=0.01,
        logging_dir="./logs",
        load_best_model_at_end=True
    )

    # ------------------------
    # 10. å®šä¹‰ Trainer
    # ------------------------
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset["test"],
    )

    # ------------------------
    # 11. å¼€å§‹è®­ç»ƒ
    # ------------------------
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.train()

    # ------------------------
    # 12. ä¿å­˜æ¨¡å‹å’Œæ ‡ç­¾ç¼–ç 
    # ------------------------
    os.makedirs("./model", exist_ok=True)
    model.save_pretrained("./model")
    tokenizer.save_pretrained("./model")
    joblib.dump(label_encoder, "./model/label_encoder.pkl")

    print("âœ… è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ° ./model ç›®å½•")

if __name__ == "__main__":
    main()